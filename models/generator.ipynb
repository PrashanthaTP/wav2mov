{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Generator based on UNET architecture \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms as vtransforms\n",
    "\n",
    "from wav2mov.core.models.base_model import BaseModel\n",
    "from wav2mov.models.utils import init_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DoubleConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Two convolution layers one followed by another\n",
    "    Uses kernel size of 3 and stride of 1 and padding 0\n",
    "    each convolution operation is followed by a relu activation\n",
    "\n",
    "    After each convolution operation the new height and width reduce by two units\n",
    "    + H_out = H_in - 2 \n",
    "    +  W_out = W_in - 2\n",
    "\n",
    "    Example:\n",
    "            >>> enc_block = DoubleConvBlock(1, 64)\n",
    "            >>> x  = torch.randn(1, 1, 572, 572)\n",
    "            >>> print(enc_block(x).shape)\n",
    "            >>> torch.Size([1, 64, 568, 568])\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('double conv block',x.shape,type(x),x.device,next(self.parameters()).device)\n",
    "        return self.relu(self.conv2(self.relu(self.conv1(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Contracting path of the UNET \n",
    "    It extracts meaningful feature map from an input image.As is standard practice \n",
    "    for a CNN , the Encoder,doubles the number of channels at everystep and halves \n",
    "    the spatial dimenstion \n",
    "\n",
    "    From the paper :\n",
    "            \"The contractive path consists of the repeated application of \n",
    "            two 3x3 convolutions (unpadded convolutions),\n",
    "            each followed by a rectified linear unit (ReLU) and\n",
    "            a 2x2 max pooling operation with stride 2 for downsampling.\n",
    "            At each downsampling step we double the number of feature channels.\"\n",
    "\n",
    "    Example:\n",
    "\n",
    "            >>> encoder = Encoder()\n",
    "            >>> # input image\n",
    "            >>> x    = torch.randn(1, 3, 572, 572)\n",
    "            >>> ftrs = encoder(x)\n",
    "            >>> for ftr in ftrs: print(ftr.shape)\n",
    "            >>> torch.Size([1, 64, 568, 568])\n",
    "            >>> torch.Size([1, 128, 280, 280])\n",
    "            >>> torch.Size([1, 256, 136, 136])\n",
    "            >>> torch.Size([1, 512, 64, 64])\n",
    "            >>> torch.Size([1, 1024, 28, 28])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chs=(3, 64, 128, 256, 512, 1024)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList(\n",
    "            [DoubleConvBlock(chs[i], chs[i+1]) for i in range(len(chs)-1)]\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        filters = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            filters.append(x)\n",
    "            x = self.pool(x)\n",
    "        return filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The expansive path of the UNET \n",
    "    From the paper:\n",
    "            Every step in the expansive path consists of an upsampling \n",
    "            of the feature map followed by a 2x2 convolution (“up-convolution”) \n",
    "            that halves the number of feature channels, a concatenation with \n",
    "            the correspondingly cropped feature map from the contracting path, \n",
    "            and two 3x3 convolutions, each followed by a ReLU. \n",
    "            The cropping is necessary due to the loss of border pixels \n",
    "            in every convolution.\n",
    "\n",
    "    Example:\n",
    "            >>> decoder = Decoder()\n",
    "            >>> x = torch.randn(1, 1024, 28, 28)\n",
    "            >>> decoder(x, encoder_out[::-1][1:]).shape\n",
    "\n",
    "            >>> (torch.Size([1, 64, 388, 388])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, up_chs=(1026, 512, 256, 128, 64),dec_chs=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.up_chs = up_chs\n",
    "        self.dec_chs = dec_chs\n",
    "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(self.up_chs[i], self.up_chs[i+1], 2, 2)\n",
    "                                      for i in range(len(self.up_chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([DoubleConvBlock(self.dec_chs[i], self.dec_chs[i+1])\n",
    "                                         for i in range(len(self.dec_chs)-1)])\n",
    "\n",
    "    def forward(self, x, encoded_features):\n",
    "        \"\"\"\n",
    "        >>> up torch.Size([1, 512, 56, 56])=>[(H-1)/s] + k -2*p = [(64-1)/2]+2-2*0 = \n",
    "        >>> enc torch.Size([1, 512, 64, 64])=>croppped to 56x56\n",
    "        >>> cat torch.Size([1, 1024, 56, 56]) #cat(up,conv)\n",
    "        \"\"\"\n",
    "        for i in range(len(self.up_chs)-1):\n",
    "            x = self.upconvs[i](x)\n",
    "            cropped_enc_feat = self.crop(encoded_features[i], x)\n",
    "            # print('up',x.shape)\n",
    "            x = torch.cat([x, cropped_enc_feat], dim=1)\n",
    "            # print('enc',encoded_features[i].shape)\n",
    "            # print('cat',x.shape)\n",
    "\n",
    "            x = self.dec_blocks[i](x)\n",
    "        return x\n",
    "\n",
    "    def crop(self, encoded_features, x):\n",
    "        _, _, H, W = x.shape\n",
    "        return vtransforms.CenterCrop([H, W])(encoded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Generator(BaseModel):\n",
    "    \"\"\"    Unet Architecture    https://arxiv.org/abs/1505.04597\n",
    "\n",
    "            Kernel 4x4,stride 2, padding 1\n",
    "            Leaky Relu in Encoder and ReLU in Decoder(Tanh at the output)\n",
    "\n",
    "            >>> enc_filters = self.encoder(x)\n",
    "            >>> out = self.decoder(enc_filters[::-1][0],enc_filters[::-1][1:])\n",
    "            >>> out = self.head(out)\n",
    "            >>> if self.retain_dim:\n",
    "            >>>         out = F.interpolate(out,self.img_dim)\n",
    "            >>> return out\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.img_dim = self.hparams['img_dim']\n",
    "        self.retain_dim = self.hparams['retain_dim']\n",
    "\n",
    "        enc_chs = [self.hparams['in_channels']] + self.hparams['enc_chs']\n",
    "        dec_chs = self.hparams['dec_chs']\n",
    "        up_chs = self.hparams['up_chs']\n",
    "        self.encoder = Encoder(enc_chs)\n",
    "        self.decoder = Decoder(up_chs=up_chs,dec_chs=dec_chs)\n",
    "        \n",
    "        init_net(self.encoder)\n",
    "        init_net(self.decoder)\n",
    "        \n",
    "        self.head = nn.Sequential(nn.Conv2d(dec_chs[-1], self.hparams['in_channels'], 1),nn.Tanh())  # ((388-3)/1)+1 = 385\n",
    "\n",
    "    def forward(self, frame_img, audio_noise):\n",
    "        enc_filters = self.encoder(frame_img)\n",
    "        # channel wise catenation\n",
    "        # print(audio_noise.shape,enc_filters[::-1][0].shape)\n",
    "        enc_filters = enc_filters[::-1]\n",
    "        enc_filters[0] = torch.cat([enc_filters[0], audio_noise], dim=1)\n",
    "        # print(enc_filters[0].shape)\n",
    "        out = self.decoder(enc_filters[0], enc_filters[1:])\n",
    "        out = self.head(out)\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, self.img_dim)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class AudioEnocoder(nn.Module):\n",
    "    \"\"\" \n",
    "       >>> x = x.reshape(x.shape[0], 1, -1)\n",
    "       >>> x = self.conv(x)\n",
    "       >>> x = self.fc(x.reshape(x.shape[0],-1))\n",
    "       >>> return x #shape (batch_size,28*28)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, 3, 3),  # ((666-3)/3)+1 = 663/3 +1 = 222\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 1, 3, 3),  # ((222-3)/3)+1 = 219/3+1 = 73+1 = 74\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv1d(128,256,4,1),#(74-4)/1 +1 = 71\n",
    "            # nn.ReLU(),\n",
    "            # nn.Conv1d(256,512,4,1),#(71-4)/1+1 = 68\n",
    "            # nn.ReLU(),\n",
    "            # nn.Conv1d(512,1,5,1),#(68-5)/1+1 = 64\n",
    "            # nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(nn.Linear(74, 128), nn.ReLU(),\n",
    "                                nn.Linear(128, 64), nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape) \n",
    "        x = x.reshape(x.shape[0], 1, -1)\n",
    "        x = self.conv(x)\n",
    "        # print(x.shape) #=> (1,1,74)\n",
    "        x = self.fc(x.reshape(x.shape[0], -1))\n",
    "  \n",
    "        return x  # shape (batch_size,28*28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NoiseGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "        >>> noise = torch.randn(1,100)\n",
    "        >>> return self.fc(noise)#shape (batch_size,28*28) \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,hparams):\n",
    "        super().__init__()\n",
    "        self.device = hparams['device']\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(100, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),#out dim is based on encoder part bottlenck of generator\n",
    "            nn.ReLU()\n",
    "            # nn.Linear(64, 28*28),\n",
    "            # nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self):\n",
    "        noise = torch.randn(1, 100).to(self.device)\n",
    "        return self.fc(noise)  # shape (batch_size,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class GeneratorBW(BaseModel):\n",
    "    \"\"\" \n",
    "        >>> x = torch.cat([self.audio_enc(audio).reshape(-1,1,28,28),self.noise_enc().reshape(-1,1,28,28)],dim=1)\n",
    "        >>> return self.identity_enc(frame_img,x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.audio_enc = AudioEnocoder()\n",
    "        self.noise_enc = NoiseGenerator(self.hparams)\n",
    "        self.identity_enc = Generator(self.hparams)\n",
    "\n",
    "        init_net(self.audio_enc)\n",
    "        init_net(self.noise_enc)\n",
    "        init_net(self.identity_enc)\n",
    "        \n",
    "    def forward(self, audio, frame_img):\n",
    "        # batch_size = audio.shape[0]\n",
    " \n",
    "        x = torch.cat([self.audio_enc(audio).reshape(-1, 1, 8, 8),\n",
    "                       self.noise_enc().reshape(-1, 1, 8, 8)], dim=1)\n",
    "        # x = self.audio_enc(audio).reshape(-1,1,8,8)\n",
    "        return self.identity_enc(frame_img, x)\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.hparams['lr'], betas=(0.5, 0.999))\n"
   ]
  }
 ]
}