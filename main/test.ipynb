{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from numpy import save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms as vtransforms\n",
    "from torchvision import utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from wav2mov.models.generator import GeneratorBW\n",
    "from wav2mov.models.wav2mov_2 import Wav2MovBW\n",
    "from wav2mov.main.data import get_dataloaders\n",
    "from wav2mov.utils.audio import StridedAudio\n",
    "from wav2mov.utils.plots import show_img,save_gif\n",
    "# from wav2mov.\n",
    "\n",
    "\n",
    "# GEN_CHECKPOINT = r'E:\\Users\\VS_Code_Workspace\\Python\\VirtualEnvironments\\wav2mov\\wav2mov\\runs\\Run_18_3_2021__2_4\\gen_Run_18_3_2021__2_4.pt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_model(options,hparams, config, logger):\n",
    "    checkpoint = options.model_path\n",
    "    loaders,mean,std= get_dataloaders(config, hparams, shuffle=True)\n",
    "    # val_dl = loaders.val\n",
    "    # for i in range(25):\n",
    "    #     sample = next(iter(val_dl))\n",
    "    #     sample = next(iter(val_dl))\n",
    "    \n",
    "    sample = next(iter(loaders.val))\n",
    "        \n",
    "    stride = hparams['data']['audio_sf']//hparams['data']['video_fps']\n",
    "    num_channels = hparams['img_channels']\n",
    "    mean,std = 0.5,0.5\n",
    "    \n",
    "    transforms = vtransforms.Compose(\n",
    "        [\n",
    "        vtransforms.Grayscale(1),\n",
    "         vtransforms.Resize((hparams['img_size'], hparams['img_size'])),\n",
    "         vtransforms.Normalize(mean,std)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    strided_audio = StridedAudio(stride=stride, coarticulation_factor=0)\n",
    "  \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "    model = GeneratorBW(hparams=hparams['gen'])\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    if 'state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "       \n",
    "\n",
    "    audio, video = sample  \n",
    "\n",
    "\n",
    "    video = video.permute(0, 1, 4, 2, 3)\n",
    "    video=video/255  #! important\n",
    "    audio, video = audio.to(device), video.to(device)\n",
    "\n",
    "    get_framewise_audio = strided_audio.get_frame_wrapper(audio)\n",
    " \n",
    "    still_image = video[:, -25, :, :, :]\n",
    "    still_image = transforms(still_image)\n",
    "    # vutils.save_image(still_image,'still_image.png')\n",
    "    # return\n",
    "    \n",
    "    num_video_frames = video.shape[1]\n",
    "    \n",
    "    num_audio_frames = audio.shape[1]//stride\n",
    "    \n",
    "    limit = min(num_audio_frames, num_video_frames)\n",
    "    \n",
    "    fake_frames = []\n",
    "    real_frames = []\n",
    "    logger.info('started')\n",
    "    for idx in range(limit):\n",
    "        video_frame = video[:, idx, ...]  # ellipsis\n",
    "        audio_frame, _ = get_framewise_audio(idx)\n",
    "        video_frame = transforms(video_frame)\n",
    "        # print(video_frame.shape)\n",
    "        # show_img(video_frame)\n",
    "        # return\n",
    "        real_frames.append((video_frame*std +mean)*255)\n",
    "        fake_frame = model(audio_frame, still_image)\n",
    "        fake_frames.append((fake_frame.detach()*std +mean)*255)  \n",
    "        \n",
    "        #normalization involves (x-mean)/srd : -1 and 1\n",
    "        \n",
    "        logger.info(f'[{idx+1:2d}/{limit}] fake frame generated | shape {fake_frame.shape}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    fake_frames = torch.cat(fake_frames,dim=0)\n",
    "    real_frames = torch.cat(real_frames,dim=0)\n",
    "\n",
    "    logger.info(f'fake_frames shape : {fake_frames.shape}')\n",
    "    version = os.path.basename(options.model_path).strip('gen').split('.')[0]\n",
    "    \n",
    "    \n",
    "    out_dir  = os.path.join(config['out_dir'],version)\n",
    "    os.makedirs(out_dir,exist_ok=True)\n",
    "    gif_name_fake = f'fake_frames_{version}.gif'\n",
    "    gif_name_real = f'real_frames_{version}.gif'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
