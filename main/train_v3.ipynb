{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Training script : can start from previous checkpoint\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.types import Number\n",
    "from torchvision import transforms as vtransforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from wav2mov.models.wav2mov_2 import Wav2MovBW\n",
    "from wav2mov.main.data import get_dataloaders\n",
    "from wav2mov.utils.audio import StridedAudio\n",
    "from wav2mov.utils.misc import AverageMetersList, ProgressMeter\n",
    "\n",
    "from wav2mov.logger import TensorLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "DISPLAY_EVERY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_tensor_logger(config):\n",
    "    tensor_logger = TensorLogger(config['runs_dir'])\n",
    "    writer_names = ['writer_gen', 'writer_sync_disc',\n",
    "                    'writer_seq_disc', 'writer_id_disc']\n",
    "    tensor_logger.add_writers(writer_names)\n",
    "    return tensor_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tensor_logger(options, config):\n",
    "    return setup_tensor_logger(config) if options.log in ['y', 'yes'] else None\n",
    "\n",
    "\n",
    "def add_to_board(tensor_logger, losses, global_step, scalar_type):\n",
    "    for name, value in losses.items():\n",
    "        writer_name = 'writer_' + name\n",
    "        tensor_logger.add_scalar(\n",
    "            writer_name, scalar_type+'_'+name, value, global_step)\n",
    "\n",
    "\n",
    "\n",
    "def add_img_grid(tensor_logger, img_grid, global_step, img_type):\n",
    "    tensor_logger.add_image(img_grid, img_type, global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_train_dl(config, hparams):\n",
    "    loaders, mean, std = get_dataloaders(config, hparams, shuffle=True)\n",
    "    train_dl = loaders.train\n",
    "    return train_dl, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_transforms(img_size, img_channels):\n",
    "    transforms = vtransforms.Compose(\n",
    "        [\n",
    "            vtransforms.Grayscale(1),\n",
    "         vtransforms.Resize(img_size),\n",
    "         vtransforms.Normalize([0.5]*img_channels, [0.5]*img_channels)\n",
    "         ])\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_device(hparams):\n",
    "    if hparams['device'] == 'cpu':\n",
    "        device = torch.device('cpu')\n",
    "    else:\n",
    "        device = torch.device(\n",
    "            'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_checkpoint(options):\n",
    "    return torch.load(options.model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_meters(hparams):\n",
    "    num_epochs = hparams['num_epochs']\n",
    "    loss_meters = AverageMetersList(('id_disc', 'sync_disc', 'seq_disc', 'gen'),\n",
    "                                    fmt=':0.4f')  # per video\n",
    "    epoch_loss_meters = AverageMetersList(('id_disc', 'sync_disc', 'seq_disc', 'gen'),\n",
    "                                          fmt=':0.4f')  # per epoch\n",
    "\n",
    "    progress_meter = ProgressMeter(num_epochs, epoch_loss_meters.as_list())\n",
    "\n",
    "    return loss_meters, epoch_loss_meters, progress_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(options, hparams, config, logger):\n",
    "    train_dl, mean, std = get_train_dl(config, hparams)\n",
    "\n",
    "    img_channels = hparams['img_channels']\n",
    "    img_size = hparams['img_size']\n",
    "    num_epochs = hparams['num_epochs']\n",
    "\n",
    "    transforms = get_transforms((img_size, img_size), img_channels)\n",
    "\n",
    "    stride = hparams['data']['audio_sf']//hparams['data']['video_fps']\n",
    "    strided_audio = StridedAudio(stride=stride, coarticulation_factor=0)\n",
    "    device = get_device(hparams)\n",
    "\n",
    "    logger.info(f'option : num_videos : {options.num_videos} | mean : {mean} std: {std} | stride :{stride}')\n",
    "\n",
    "    start_epoch = 0\n",
    "    model = Wav2MovBW(config, hparams, logger)\n",
    "    if getattr(options,'model_path',None) is not None:\n",
    "        logger.debug(f'Loading pretrained weights : {config.version}')\n",
    "       \n",
    "        prev_epoch = model.load(checkpoint_dir=options.model_path)\n",
    "        if prev_epoch is not None:\n",
    "            start_epoch = prev_epoch+1\n",
    "        logger.debug(f'weights loaded successfully: {config.version}')\n",
    " \n",
    "    NUM_VIDEOS = options.num_videos if options.num_videos is not None else len(train_dl)\n",
    "    ################################\n",
    "    # Setup loggers and loss meters\n",
    "    ################################\n",
    "    tensor_logger = get_tensor_logger(options, config)\n",
    "    loss_meters, epoch_loss_meters, progress_meter = get_meters(hparams)\n",
    "\n",
    "    STILL_IMAGE_IDX = 5\n",
    "    logger.info(f'{STILL_IMAGE_IDX}th frame from last of every video is considered as reference image for the generator')\n",
    "    logger.info(f'Training started on {device}')\n",
    "    steps = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.on_train_start()\n",
    "\n",
    "    ################################\n",
    "    # Training loop\n",
    "    ################################\n",
    "    for epoch in range(start_epoch,num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_loss_meters.reset()\n",
    "        for batch_idx, sample in enumerate(train_dl):\n",
    "            batch_start_time = time.time()\n",
    "            loss_meters.reset()\n",
    "\n",
    "            audio, video = sample  # channel axis is the last one\n",
    "\n",
    "            # channel axis must be after the batch size,frame_count\n",
    "            #change video shape from (B,F,H,W,C) to (B,F,C,H,W)\n",
    "            video = video.permute(0, 1, 4, 2, 3)\n",
    "\n",
    "            audio, video = audio.to(device), video.to(device)\n",
    "            video = video/255  # !important\n",
    "            get_framewise_audio = strided_audio.get_frame_wrapper(audio)\n",
    "            # video is of shape(batch_size,num_video_frames,channels,H,W)\n",
    "\n",
    "            still_image = video[:, -STILL_IMAGE_IDX, :, :, :]\n",
    "            still_image = transforms(still_image)\n",
    "            model.set_condition(still_image)\n",
    "\n",
    "            # video is of shape : (batch_size,num_frames,channels,img_height,img_width)\n",
    "            num_video_frames = video.shape[1]\n",
    "\n",
    "            num_audio_frames = audio.shape[1]//stride\n",
    "            limit = min(num_audio_frames, num_video_frames)\n",
    "\n",
    "            for idx in range(limit):\n",
    "                video_frame = video[:, idx, ...]  # ellipsis\n",
    "                audio_frame, _ = get_framewise_audio(idx)\n",
    "                video_frame = transforms(video_frame)\n",
    "\n",
    "                model.set_input(audio_frame, video_frame)\n",
    "                losses = model.optimize_parameters()\n",
    "\n",
    "                loss_meters.update(losses, n=1)\n",
    "                steps += 1\n",
    "\n",
    "            losses = model.optimize_sequence()\n",
    "            gen_loss = losses.pop('gen', 0)\n",
    "            loss_meters.update(losses, n=1)\n",
    "            loss_meters.get('gen').add(gen_loss)\n",
    "\n",
    "            batch_duration = time.time()-batch_start_time\n",
    "            # for every video as batch size is 1\n",
    "            if tensor_logger is not None:\n",
    "                add_to_board(tensor_logger, loss_meters.average(),\n",
    "                             steps, scalar_type='loss')\n",
    "\n",
    "            epoch_loss_meters.update(loss_meters.average(), n=1)\n",
    "\n",
    "            logger.debug( f'\\nEpoch {epoch+1}/{num_epochs} [{batch_duration:0.2f} s or {batch_duration/60:0.2f} min] : video num {batch_idx+1}/{len(train_dl)}')\n",
    "\n",
    "            logger.debug(f'audio shape : {audio.shape} | video shape : {video.shape}')\n",
    "\n",
    "            logger.debug(loss_meters)\n",
    "\n",
    "            if (batch_idx+1) == NUM_VIDEOS:\n",
    "                break\n",
    "            if (batch_idx) % 20 == 0:\n",
    "                model.save(epoch=epoch)\n",
    "                hparams.save(config['params_checkpoint_fullpath'])\n",
    "\n",
    "        logger.info(progress_meter.get_display_str(epoch+1))\n",
    "        model.save(epoch=epoch)\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        logger.info( f'[Epoch {epoch+1}/{num_epochs}] {epoch_duration:0.2f} seconds or {epoch_duration/60:0.2f} minutes')\n",
    "\n",
    "    hparams.save(config['params_checkpoint_fullpath'])\n",
    "    model.on_train_end()\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_train_time = end_time - start_time\n",
    "    logger.info('Trainging successfully completed')\n",
    "    logger.info(f'Time taken {total_train_time:0.2f} seconds or {total_train_time/60:0.2f} minutes')\n",
    "    logger.info(str(model.to('cpu')))\n",
    "    \n",
    "    model_path = config['gen_checkpoint_fullpath']\n",
    "    with open('trained.txt', 'w') as file:\n",
    "        file.write(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
